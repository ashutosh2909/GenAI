# -*- coding: utf-8 -*-
"""News Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BqR5V89tPzLsirVfvpj78TKDeb7bS1ye
"""

# !pip install gnews
!pip install lxml[html_clean] newspaper3k

import numpy as np
import pandas as pd
import yfinance as yf
from newspaper import Article
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)

from transformers import BertTokenizer, BertForSequenceClassification

finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')

ticker = yf.Ticker('RELIANCE.NS')

news_lst = []
pub_date_lst = []
url_lst = []
for i in range(len(ticker.news)):
  # print(i)
  news_lst.append(ticker.news[i]['content']['summary'])
  pub_date_lst.append(ticker.news[i]['content']['pubDate'])
  url_lst.append(ticker.news[i]['content']['clickThroughUrl']['url'])

news_df = pd.DataFrame(news_lst)
news_df.columns = ['news']
news_df['pubDate'] = pub_date_lst
news_df['url'] = url_lst

# Create empty columns first
news_df['article_title'] = None
news_df['article_text'] = None

for idx, url in news_df['url'].items():
    try:
        article = Article(url)
        article.download()
        article.parse()

        news_df.at[idx, 'article_title'] = article.title
        news_df.at[idx, 'article_text'] = article.text

    except Exception as e:
        print(f"Failed at index {idx}: {e}")

def summarize_article_llama(article, tokenizer, model):
    # Handle empty or invalid text
    if not isinstance(article, str) or len(article.strip()) == 0:
        return ""

    # Chat-style prompt
    prompt = f"""
<|system|>
You are a financial news analyst.
Summarize the following article clearly and concisely.
Focus on key sentiment, and market impact.
<|user|>
{article}
<|assistant|>
"""

    try:
        # Tokenize
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(model.device)

        # Generate summary
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.3,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                eos_token_id=tokenizer.eos_token_id
            )

        # Decode
        summary = tokenizer.decode(
            output_ids[0],
            skip_special_tokens=True
        )

        # Remove prompt echo
        return summary.split("<|assistant|>")[-1].strip()

    except Exception as e:
        print(f"Summarization failed: {e}")
        return ""


# Apply to entire dataframe
news_df["news_summarized"] = news_df["article_text"].apply(
    lambda x: summarize_article_llama(x, tokenizer, model)
)

# Preview
news_df.head()

X = news_df['news_summarized'].to_list()
labels = {0:'neutral', 1:'positive', 2:'negative'}

sent_val = list()
scores = []
for x in X:
    inputs = tokenizer(x, return_tensors="pt", padding=True, truncation=True, max_length=512)
    outputs = finbert(**inputs)[0]
    # print(outputs.detach().numpy())

    val = labels[np.argmax(outputs.detach().numpy())]
    print(x, '----', val)
    print('#######################################################')
    sent_val.append(val)
    scores.append(outputs.tolist()[0])

scores_df = pd.DataFrame(scores, columns=['neutral', 'positive', 'negative'])

news_df['sentiment'] = sent_val
news_df['pubDate'] = pd.to_datetime(news_df['pubDate'])
news_df['pubDate'] = news_df['pubDate'].dt.strftime('%Y-%m-%d')
news_df_final = pd.concat([news_df, scores_df], axis = 1)
news_df_final

